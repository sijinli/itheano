# Modified from 0028
# This version is the first one to try the batchnorm and elemscale
#---------------------------------------------------
# Modified from 0020
# Fixed 0020 bug of layer_with_weights (0020 has duplicate weights)
# This version will init the weights from 0027_1
# To be specific, 
# Action 14
# /opt/visal/tmp/for_sijin/Data/saved/theano_models/2015_02_28_FCJ0_act_14_cosine_test_1/
#--------------------------------------------------
# Modified from 0013
# This network will take the fc_j0 feature as inputs
#--------------------------------------------------

[l_img_feature]
type=data
inputs=s_img_feature
input_dims=1024
neuron=linear[0.1, 0]
# Max = 4.529443e+01 	 median = 0.000000e+00 	 min = 0.000000e+00
# avg = 1.279730e+00 , std = 3.207543e+00

[l_target]
type=data
inputs=s_target
input_dims=51

[l_candidate]
type=data
inputs=s_candidate
input_dims=51


[l_gt_margin]
type=data
inputs=s_gt_margin
input_dims=1

[l_candidate_margin]
type=data
inputs=s_candidate_margin
input_dims=1

[img_feature_norm]
type=batchnorm
inputs=l_img_feature

[img_feature_scale]
type=elemscale
inputs=img_feature_norm
wd=0.01
initW=0.01
initb=0
initWfunc=initfunc.constant(1)
initbfunc=initfunc.constant(0)
epsW=0.0005
epsB=0.002

[fc0_img]
type=fc
inputs=img_feature_scale
output_dims=1024
wd=0.01
initW=0.01
initb=0
neuron=relu2
epsW=0.0005
epsB=0.002
initWfunc=initfunc.gwfp(/opt/visal/tmp/for_sijin/Data/saved/theano_models/2015_02_28_FCJ0_act_14_cosine_test_1/,fc_f0)
initBfunc=initfunc.gbfp(/opt/visal/tmp/for_sijin/Data/saved/theano_models/2015_02_28_FCJ0_act_14_cosine_test_1/,fc_f0)

[net1_fc0_jt]
type=fc
inputs=l_target
output_dims=1024
wd=0.01
initW=0.1
initb=0
neuron=relu2
epsW=0.0005
epsB=0.002
initWfunc=initfunc.gwfp(/opt/visal/tmp/for_sijin/Data/saved/theano_models/2015_02_28_FCJ0_act_14_cosine_test_1/,fc_encode0)
initBfunc=initfunc.gbfp(/opt/visal/tmp/for_sijin/Data/saved/theano_models/2015_02_28_FCJ0_act_14_cosine_test_1/,fc_encode0)



[net1_fc1]
type=fc
inputs=fc0_img,net1_fc0_jt
output_dims=512
wd=0.00001,0.00001
initW=0.01,0.01
initb=0
epsW=0.0005,0.0005
epsB=0.002
neuron=relu2

[net1_fc2]
type=fc
inputs=net1_fc1
output_dims=1024
wd=0.00001
initW=0.1
initb=0
epsW=0.0005
epsB=0.002
neuron=relu2


[net1_fc2_dropout]
type=dropout
inputs=net1_fc2
keep=0.25


[net1_fc3]
type=fc
inputs=net1_fc2_dropout
output_dims=1
wd=0.00001
initW=0.1
initb=1
epsW=0.0005
epsB=0.002

[net2_fc0_jt]
type=fc
inputs=l_candidate
output_dims=1024
wd=0.00001
initW=0.1
initb=0
neuron=relu2
weightSource=net1_fc0_jt[0]
biasSource=net1_fc0_jt[0]



[net2_fc1]
type=fc
inputs=fc0_img,net2_fc0_jt
output_dims=512
wd=0.00001,0.00001
initW=0.1,0.1
initb=0
weightSource=net1_fc1[0],net1_fc1[1]
biasSource=net1_fc1
neuron=relu2

[net2_fc2]
type=fc
inputs=net2_fc1
output_dims=1024
wd=0.00001
initW=0.01
initb=0
weightSource=net1_fc2
biasSource=net1_fc2
neuron=relu2

[net2_fc2_dropout]
type=dropout
inputs=net2_fc2
keep=0.25


[net2_fc3]
type=fc
inputs=net2_fc2_dropout
output_dims=1
wd=0.00001
initW=0.01
initb=1
weightSource=net1_fc3
biasSource=net1_fc3


[net1_score]
type=eltsum
inputs=net1_fc3, l_gt_margin
coeffs=1,1

[net2_score]
type=eltsum
inputs=net2_fc3, l_candidate_margin
coeffs=1,1

[net2_mmcost]
type=cost.maxmargin
inputs=net1_score,net2_score
neuron=relu2

[network1]
type=network
data_idx=0,1,0
cost_layer_names=
data_layer_names=l_img_feature, l_target, l_gt_margin
output_layer_names=net1_score
layer_with_weights=net1_fc1,net1_fc2, net1_fc3,fc0_img,net1_fc0_jt

[network2]
type=network
data_idx=0,1,0
cost_layer_names=net2_mmcost
data_layer_names=l_img_feature, l_target, l_candidate, l_gt_margin,l_candidate_margin
output_layer_names=
layer_with_weights=fc0_img,net1_fc0_jt, net1_fc1, net1_fc2, net1_fc3, img_feature_scale

