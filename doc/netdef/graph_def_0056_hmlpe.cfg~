# modified from 0055
# [ ] Add one more img transform layers fc_1_add
# [ ]
#------------------------------------------------
# Modified from 0054
#  [ ] doesn't learn conv1,conv2,conv3
#  [ ] doesn't init the weights for both prediction and embedding
#-----------------------------------------------
# Modified from 0042
# In this version, I will try 
# to seperate the image and pose transform layer
# and use pose estimation as outputs
# The network are forced to learn discriminative features via structure prediction
# --------------------------------------------
# Modified from 0041
# In this version, I will try to use very smaller number of nodes in the last layer
# So that try to reduce the overfitting problem
#---------------------------------------------
# Modified from 0037
# all the wd is set to 0.01
# Then the network will be trained so that the regularization term will not be 
# divided by n
# ---------------------------------------------
# Modified from 0034_corrected_version
# Here I 
# ---------------------------------------------
# It is strange the performance of current saved 0034 version is 
# worse than its "parent"" network
# I will save a check network and check its performance
# So all the learning rates here is zero
# Modified from 0023
#---------------------------------------------------
# Modified from 0022
# This version  I remove sqdiff cost from the network
# 
# Modified from 0021
# This version will init the weights 
#------------------------------------
# Combination  of 0012 and 0020
# 2015_02_02_acm_act_14_exp_2_19_graph_0012/
# FCJ0_act_14_graph_0020_test
#-------------------------------------


[imgdata]
type=data
inputs=s_imgdata
input_dims=(3, 112,112)
#channel, height, width

[joints]
type=data
inputs=s_joints
input_dims=51

[candidate_joints]
type=data
inputs=s_candidate_joints
input_dims=51


[l_gt_margin]
type=data
inputs=s_gt_margin
input_dims=1

[l_candidate_margin]
type=data
inputs=s_candidate_margin
input_dims=1



[conv1]
type=conv
inputs=imgdata
sizeX=9
sizeY=9
filters=32
strideX=1
strideY=1
board_mode=valid
initW=0.00005
initb=0
wd=0
neuron=relu2
epsW=0.0005
epsB=0.002
initWfunc=initfunc.gwfp(/opt/visal/tmp/for_sijin/Data/saved/theano_models/2015_02_02_acm_act_14_exp_2_19_graph_0012/)
initBfunc=initfunc.gbfp(/opt/visal/tmp/for_sijin/Data/saved/theano_models/2015_02_02_acm_act_14_exp_2_19_graph_0012/)
# 32 @ 104 x 104

[pool1]
type=pool
inputs=conv1
sizeX=3
sizeY=3
strideX=2
strideY=2
pooling_type=max
# 32 @ 52 x 52

[conv2]
type=conv
inputs=pool1
sizeX=5
sizeY=5
filters=64
strideX=1
strideY=1
board_mode=valid
initW=0.001
initb=0
wd=0
neuron=relu2
epsW=0.0005
epsB=0.002
initWfunc=initfunc.gwfp(/opt/visal/tmp/for_sijin/Data/saved/theano_models/2015_02_02_acm_act_14_exp_2_19_graph_0012/)
initBfunc=initfunc.gbfp(/opt/visal/tmp/for_sijin/Data/saved/theano_models/2015_02_02_acm_act_14_exp_2_19_graph_0012/)
# 64 @ 48 x 48

[pool2]
type=pool
inputs=conv2
sizeX=3
sizeY=3
strideX=2
strideY=2
pooling_type=max
# 64 @ 24 x 24


[conv3]
type=conv
inputs=pool2
sizeX=5
sizeY=5
filters=64
strideX=1
strideY=1
board_mode=valid
initW=0.001
initb=0
wd=0
epsW=0.0005
epsB=0.002
initWfunc=initfunc.gwfp(/opt/visal/tmp/for_sijin/Data/saved/theano_models/2015_02_02_acm_act_14_exp_2_19_graph_0012/)
initBfunc=initfunc.gbfp(/opt/visal/tmp/for_sijin/Data/saved/theano_models/2015_02_02_acm_act_14_exp_2_19_graph_0012/)
#neuron=relu
# 64 @ 20 x 20

[pool3]
type=pool
inputs=conv3
sizeX=3
sizeY=3
strideX=2
strideY=2
pooling_type=max
# 64 @ 10 x 10


[fc_j0]
type=fc
inputs=pool3
output_dims=1024
wd=0.001
initW=0.001
epsW=0.0005
epsB=0.002
initb=0
neuron=relu2



[fc_j0_dropout]	
type=dropout
inputs=fc_j0
keep=0.25

[fc_j1]
type=fc
inputs=fc_j0_dropout
output_dims=2048
wd=0.01
initW=0.001
epsW=0.0005
epsB=0.002
initb=0
neuron=relu2


[fc_j1_add]
type=fc
inputs=fc_j1
output_dims=1024
wd=0.01
initW=0.001
epsW=0.0005
epsB=0.002
initb=0
neuron=relu2


# This is for image feature representation
[fc_j2]
type=fc
inputs=fc_j1_add
output_dims=51
wd=0.01
initW=0.001
epsW=0.0005
epsB=0.002
initb=0
neuron=tanh[1,1]


#########################################################
[net1_fc_0]
type=fc
inputs=joints
output_dims=2048
wd=0.001
initW=0.1
initb=0
neuron=relu2
epsW=0.0005
epsB=0.002

[net1_fc_0_add]
type=fc
inputs=net1_fc_0
output_dims=1024
wd=0.00001
initW=0.1
initb=0
neuron=relu2
epsW=0.0005
epsB=0.002


###################
[net2_fc_0]
type=fc
inputs=candidate_joints
output_dims=2048
wd=0.001
initW=0.1
initb=0
neuron=relu2
weightSource=net1_fc_0
biasSource=net1_fc_0

[net2_fc_0_add]
type=fc
inputs=net2_fc_0
output_dims=1024
wd=0.00001
initW=0.1
initb=0
neuron=relu2
weightSource=net1_fc_0_add
biasSource=net1_fc_0_add



[net1_prod]
type=dotprod
inputs=net1_fc_0_add,fc_j1_add

[net2_prod]
type=dotprod
inputs=net2_fc_0_add,fc_j1_add

[net1_score]
type=eltsum
inputs=net1_prod, l_gt_margin
coeffs=1,1

[net2_score]
type=eltsum
inputs=net2_prod, l_candidate_margin
coeffs=1,1


###################################
[sqdiffcost]
type=cost.sqdiff
inputs=joints,fc_j2
coeff=0.5


[net2_mmcost]
type=cost.maxmargin
inputs=net1_score,net2_score
neuron=relu2
coeff=0.5

[feature_net]
type=network
cost_layer_names=
data_layer_names=imgdata
output_layer_names=fc_j1
layer_with_weights=

[score_net]
type=network
cost_layer_names=
data_layer_names=fc_j1_add,joints,l_gt_margin
output_layer_names=net1_score
layer_with_weights=


[train_net]
type=network
data_layer_names=imgdata,joints,candidate_joints,l_gt_margin,l_candidate_margin
cost_layer_names=net2_mmcost,sqdiffcost
output_layer_names=net1_score,net2_score
layer_with_weights=net1_fc_0,fc_j0,fc_j1,fc_j2,fc_j1_add,net1_fc_0_add

