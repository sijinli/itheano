Note{
DATE: Jan 16, 2015
Network 2: The only difference between network 2 and network 1 is that I use different relu in the maxmargin layer. 
In the network 2's version, it is switch(X>0, X, 0). 
Therefore, it will have gradients only when the cost is greater than zero.
The results varies a lot
[ Here is the weights vs gradients 
net1_fc1_weights_0:	 v:+4.494107e-01 	 g: +1.700636e-02 	 [+3.784147e-02]
net1_fc1_weights_1:	 v:+7.989296e-01 	 g: +6.362572e-02 	 [+7.963870e-02]
net1_fc1_biaes_0:	 v:+1.309456e+01 	 g: +6.498061e-01 	 [+4.962413e-02]
net1_fc2_weights_0:	 v:+1.429397e+01 	 g: +3.704958e+00 	 [+2.591973e-01]
net1_fc2_biaes_0:	 v:+0.000000e+00 	 g: +0.000000e+00 	 [+nan]
]
file_saved:/public/sijinli2/ibuffer/2015-01-16/itheano_test_act14_net2_test1_2014_01_16

I find the problem. 
Because I didn't preprocess the data, so that the margin will be extremely large.
Also, it is not suitable for current network, 
}

RUN {
net1:saved to /public/sijinli2/ibuffer/2015-01-16/net1
}

RUN {
savepath:/public/sijinli2/ibuffer/2015-01-16/net2_test_for_stat_no_hold
exp_name=SP_t004_act_14
DP=mem
macid=15
JT=0002
EP=200
BSIZE=1024
run_mac=c8k$(macid)
TrainRange=0-132743
TestRange=132744-162007
K-candidate=200 
K-most-violated=0
net2:
}

RUN {DATE: Jan 21 2015
exp_name=SP_t004_act_14
DP=mem
macid=16
JT=0004
EP=200
BSIZE=1024
run_mac=c8k$(macid)
TrainRange=0-132743
TestRange=132744-162007

net4:/public/sijinli2/ibuffer/2015-01-16/net4_test_for_stat

COMMENT: This time, I use larger candidate set. So that it can search a larger space. 
}

RUN {
DATE: Feb 1 2015
/opt/visal/tmp/for_sijin/tmp/tmp_theano  <-- A mistake | I add dropout on scores | so stupid

/opt/visal/tmp/for_sijin/Data/saved/theano_models/acm_act_14_exp_2_01_31_2015_16/
}

RUN {
/opt/visal/tmp/for_sijin/Data/saved/theano_models/2015_02_01_acm_act_14_exp_2_15
The performance becomes to decrease at some points because nan appears.

DATE: Feb 1, 2015
exp_name=SP_t004_act_14
DP=mem
macid=17
JT=0010
EP=200
BSIZE=1024
run_mac=c8k$(macid)
KC=200
KMV=100
MAXNUM=100
TrainRange=0-132743
TestRange=132744-162007
save_name=$(exp_name)_$(JT)
save_folder = /opt/visal/tmp/for_sijin/Data/saved/theano_models/
/opt/visal/tmp/for_sijin/Data/saved/theano_models/SP_t004_act_14_0010
COMMENT: Dropout +  one more layer
MPJPE: 86.3369
}

RUN {
Feb 2 2015
/opt/visal/tmp/for_sijin/Data/saved/theano_models/2015_02_02_acm_act_14_exp_2_19_graph_0012/
Eval@Feb4-2015-09:40AM: mean mpjpe = 79.672750975 epoch = 71
Eval@Feb5-2015-09:44AM: mean mpjpe = 79.679860815 epoch = 120

Feb 3 2015
/opt/visal/tmp/for_sijin/Data/saved/theano_models/SP_t004_act_14_graph_0013/
With smaller keep and number nodes, it still tends to overfit.
MPJPE=87.11156 ( However, it is worse than previous performance) Random 20000
MPJPE=80.8717975359 (random, epoch 70)
}

RUN {
Feb 3 2015
exp_name=Embedding_ASM_act_14_exp_2_ACCV_fc_j0
}

RUN {
exp_name=FCJ0_act_12
DP=mem
macid=15
JT=0016
EP=200
BSIZE=128
run_mac=c8k${macid}
TrainRange=0-76047
TestRange=76048-105367
test_freq=15
save_name=2015_02_08_FCJ0_act_12_graph_0016
COMMENTS: The scale of the input is really important
COMMENTS: MPJPE@200 epach:170.924891536
}

RUN {
Feb 8
exp_name=ASM_act_3_exp_8
DP=croppedjt
macid=14
JT=0012
EP=200
BSIZE=128
run_mac=c8k$(macid)
TrainRange=0-158787
TestRange=158788-223031
/opt/visal/tmp/for_sijin/Data/saved/theano_models/2015_02_08_ASM-act_3_exp_8_14_graph_0012
COMMENTS:14 is macid

exp_name=ASM_act_4_exp_5
DP=croppedjt
macid=18
JT=0012
EP=200
BSIZE=128
run_mac=c8k$(macid)
TrainRange=0-109423
TestRange=109424-148731
/opt/visal/tmp/for_sijin/Data/saved/theano_models/2015_02_08_ASM_act_4_exp_5_18_graph_0012
}



