
Note{
DATE: Jan 16, 2015
Network 2: The only difference between network 2 and network 1 is that I use different relu in the maxmargin layer. 
In the network 2's version, it is switch(X>0, X, 0). 
Therefore, it will have gradients only when the cost is greater than zero.
The results varies a lot
[ Here is the weights vs gradients 
net1_fc1_weights_0:	 v:+4.494107e-01 	 g: +1.700636e-02 	 [+3.784147e-02]
net1_fc1_weights_1:	 v:+7.989296e-01 	 g: +6.362572e-02 	 [+7.963870e-02]
net1_fc1_biaes_0:	 v:+1.309456e+01 	 g: +6.498061e-01 	 [+4.962413e-02]
net1_fc2_weights_0:	 v:+1.429397e+01 	 g: +3.704958e+00 	 [+2.591973e-01]
net1_fc2_biaes_0:	 v:+0.000000e+00 	 g: +0.000000e+00 	 [+nan]
]
file_saved:/public/sijinli2/ibuffer/2015-01-16/itheano_test_act14_net2_test1_2014_01_16

I find the problem. 
Because I didn't preprocess the data, so that the margin will be extremely large.
Also, it is not suitable for current network, 
}

RUN {
net1:saved to /public/sijinli2/ibuffer/2015-01-16/net1
}

RUN {
savepath:/public/sijinli2/ibuffer/2015-01-16/net2_test_for_stat_no_hold
exp_name=SP_t004_act_14
DP=mem
macid=15
JT=0002
EP=200
BSIZE=1024
run_mac=c8k$(macid)
TrainRange=0-132743
TestRange=132744-162007
K-candidate=200 
K-most-violated=0
net2:
}

RUN {DATE: Jan 21 2015
exp_name=SP_t004_act_14
DP=mem
macid=16
JT=0004
EP=200
BSIZE=1024
run_mac=c8k$(macid)
TrainRange=0-132743
TestRange=132744-162007

net4:/public/sijinli2/ibuffer/2015-01-16/net4_test_for_stat

COMMENT: This time, I use larger candidate set. So that it can search a larger space. 
}

RUN {
DATE: Feb 1 2015
/opt/visal/tmp/for_sijin/tmp/tmp_theano  <-- A mistake | I add dropout on scores | so stupid

/opt/visal/tmp/for_sijin/Data/saved/theano_models/acm_act_14_exp_2_01_31_2015_16/
}

RUN {
/opt/visal/tmp/for_sijin/Data/saved/theano_models/2015_02_01_acm_act_14_exp_2_15
The performance becomes to decrease at some points because nan appears.

DATE: Feb 1, 2015
exp_name=SP_t004_act_14
DP=mem
macid=17
JT=0010
EP=200
BSIZE=1024
run_mac=c8k$(macid)
KC=200
KMV=100
MAXNUM=100
TrainRange=0-132743
TestRange=132744-162007
save_name=$(exp_name)_$(JT)
save_folder = /opt/visal/tmp/for_sijin/Data/saved/theano_models/
/opt/visal/tmp/for_sijin/Data/saved/theano_models/SP_t004_act_14_0010
COMMENT: Dropout +  one more layer
MPJPE: 86.3369

}

RUN {
Feb 2 2015
/opt/visal/tmp/for_sijin/Data/saved/theano_models/2015_02_02_acm_act_14_exp_2_19_graph_0012/
Eval@Feb4-2015-09:40AM: mean mpjpe = 79.672750975 epoch = 71
Eval@Feb5-2015-09:44AM: mean mpjpe = 79.679860815 epoch = 120
Eval@Feb11-2015-09:44AM: mean mpjpe =78.1785291767 epoch = 138


Feb 3 2015
/opt/visal/tmp/for_sijin/Data/saved/theano_models/SP_t004_act_14_graph_0013/
With smaller keep and number nodes, it still tends to overfit.
MPJPE=87.11156 ( However, it is worse than previous performance) Random 20000
MPJPE=80.8717975359 (ALL, epoch 70)
MPJPE=79.692556 (KC=20000, epoch 79)
}

RUN {
Feb 3 2015
exp_name=Embedding_ASM_act_14_exp_2_ACCV_fc_j0
}



NOTE {
exp_name=ASM_act_12_exp_4
DP=croppedjt
JT=0012
EP=200
BSIZE=128
/opt/visal/tmp/for_sijin/Data/saved/theano_models/2015_02_04_ASM_act_12_exp_4_18_graph_0012
mean mpjpe = 176.306504863
}

RUN {
exp_name=FCJ0_act_12
DP=mem
macid=15
JT=0016
EP=200
BSIZE=128
run_mac=c8k${macid}
TrainRange=0-76047
TestRange=76048-105367
test_freq=15
save_name=2015_02_08_FCJ0_act_12_graph_0016
COMMENTS: The scale of the input is really important
COMMENTS: MPJPE@200 epach:170.924891536
}

RUN {
Feb 8
exp_name=ASM_act_3_exp_8
DP=croppedjt
macid=14
JT=0012
EP=200
BSIZE=128
run_mac=c8k$(macid)
TrainRange=0-158787
TestRange=158788-223031
/opt/visal/tmp/for_sijin/Data/saved/theano_models/2015_02_08_ASM_act_3_exp_8_14_graph_0012
COMMENTS:14 is macid
mean mpjpe = 136.896386105

exp_name=ASM_act_4_exp_5
DP=croppedjt
macid=18
JT=0012
EP=200
BSIZE=128
run_mac=c8k$(macid)
TrainRange=0-109423
TestRange=109424-148731
/opt/visal/tmp/for_sijin/Data/saved/theano_models/2015_02_08_ASM_act_4_exp_5_18_graph_0012
mean mpjpe = 117.198643991
}

NOTE{
exp_name=FCJ0_act_14
DP=mem
macid=15
JT=0016
EP=200
BSIZE=128
run_mac=c8k${macid}
TrainRange=0-132743
TestRange=132744-162007
test_freq=15
save_name=2015_02_09_FCJ0_act_14_test
/opt/visal/tmp/for_sijin/Data/saved/theano_models/2015_02_09_FCJ0_act_14_test
mean mpjpe = 82.988910692
}

RUN {
Feb 14 2015
exp_name=ASM_act_15_exp_9
DP=croppedjt
macid=19
JT=0012
EP=200
BSIZE=128
run_mac=c8k$(macid)

TrainRange=0-79411
TestRange=79412-107715

test_freq=15
save_name=2015_02_14_$(exp_name)_$(macid)_graph_$(JT)
/opt/visal/tmp/for_sijin/Data/saved/theano_models/2015_02_14_ASM_act_15_exp_9_19_graph_0012
}

RUN {
exp_name=ASM_act_5_exp_10
DP=croppedjt
macid=17
JT=0012
EP=200
BSIZE=128
run_mac=c8k$(macid)

TrainRange=0-72435
TestRange=72436-103079
/opt/visal/tmp/for_sijin/Data/saved/theano_models/2015_02_14_ASM_act_5_exp_10_17_graph_0012
}

NOTE {
exp_name=FCJ0_act_14
DP=mem
macid=13
JT=0020
EP=200
BSIZE=1024
run_mac=c8k${macid}
KC=200
KMV=100
MAXNUM=100
TrainRange=0-132743
TestRange=132744-162007
/opt/visal/tmp/for_sijin/Data/saved/theano_models/FCJ0_act_14_graph_0020_test
}

NOTE {
exp_name=ASM_act_14_exp_2
DP=croppedjt
macid=13
JT=0022
EP=200
BSIZE=128
un_mac=c8k${macid}
KC=200
KMV=100
MAXNUM=10
TrainRange=0-132743
TestRange=132744-162007
save_name=DEBUG_TEST_0023
/opt/visal/tmp/for_sijin/Data/saved/theano_models/DEBUG_TEST_0022
mean mpjpe = 77.4093566823  <------ On pose prediction part

exp_name=ASM_act_14_exp_2
DP=croppedjt
macid=13
JT=0023
EP=200
BSIZE=128
run_mac=c8k${macid}
KC=200
KMV=100
MAXNUM=10
TrainRange=0-132743
TestRange=132744-162007
save_name=DEBUG_TEST_0023
/opt/visal/tmp/for_sijin/Data/saved/theano_models/DEBUG_TEST_0023
}

Record{
exp_name=FCJ0_act_14
DP=mem
macid=15
JT=0018
EP=200
BSIZE=128
run_mac=c8k${macid}
TrainRange=0-132743
TestRange=132744-162007
/opt/visal/tmp/for_sijin/Data/saved/theano_models/FCJ0_act_14_testbp
mean mpjpe = 80.5595836496
}

RUN {
exp_name=FCJ0_act_14
DP=mem
macid=15
JT=0024
EP=200
BSIZE=128
run_mac=c8k${macid}

TrainRange=0-132743
TestRange=132744-162007

test_freq=15
save_name=2015_02_17_FCJ0_act_14_again
/opt/visal/tmp/for_sijin/Data/saved/theano_models/2015_02_17_FCJ0_act_14_again/
mean mpjpe = 79.8749450703
}

RUN {
exp_name=ASM_act_14_exp_2
DP=croppedjt
macid=14
JT=0025
EP=200
BSIZE=128
run_mac=c8k$(macid)
KC=200
KMV=100
MAXNUM=10
TrainRange=0-132743
TestRange=132744-162007
DATE=2015_02_17
save_name=$(DATE)_$(exp_name)_graph_$(JT)
/opt/visal/tmp/for_sijin/Data/saved/theano_models/2015_02_17_ASM_act_14_exp_2_graph_0025
}

RUN {
exp_name=FCJ0_act_14
DP=mem
macid=15
JT=0027_1
EP=200
BSIZE=128
run_mac=c8k${macid}

TrainRange=0-132743
TestRange=132744-162007

test_freq=1
save_name=2015_02_28_FCJ0_act_14_cosine_test_1
SP_NOTE=
/opt/visal/tmp/for_sijin/Data/saved/theano_models/2015_02_28_FCJ0_act_14_cosine_test_1/
}

RUN {
exp_name=FCJ0_act_14
DP=mem
macid=13
JT=0028
EP=200
BSIZE=1024
run_mac=c8k${macid}
KC=200
KMV=1000
MAXNUM=100
TrainRange=0-132743
TestRange=132744-162007
save_name=${exp_name}_graph_${JT}_test_KMV_1000

I was trying two versions. 
##Now in processing
In the first version, I use KMV=100, on c8k16 
/opt/visal/tmp/for_sijin/Data/saved/theano_models/FCJ0_act_14_graph_0028_test_
[Random shuffled test, batch size 128, candidate 20000
Residuals for best match poses 51.6047676782,
mpjpe as mv_margin is 89.6173502302]
[Use train as test, random batch, size = 128, candidate 20000
Residuals for best match poses 16.9762548099
mpjpe as mv_margin is 24.2432484224
]

in the second version I use KMV=1000 on c8k15

}

RUN {
exp_name=FCJ0_act_14
DP=mem
macid=13
JT=0029
EP=200
BSIZE=1024
run_mac=c8k${macid}
KC=200
KMV=100
MAXNUM=100
TrainRange=0-132743
TestRange=132744-162007
save_name=${exp_name}_graph_${JT}_test_norm
EXTRA=--force-shuffle=1
/opt/visal/tmp/for_sijin/Data/saved/Test/FCJ0_act_14_graph_0029_test_norm/

[--epoch 29@72
Residuals for best match poses 16.8584867541
mpjpe as mv_margin is 34.1280347539
Residuals for best match poses 51.1792763669
mpjpe as mv_margin is 87.8447293153 
saved to /opt/visal/tmp/for_sijin/Data/saved/theano_models/FCJ0_act_14_graph_0029_test_norm_eval

Overall, the mpjpe is 88.9815219668
]
}

RUN {
exp_name=FCJ0_act_14
DP=mem
macid=13
JT=0030
EP=200
BSIZE=1024
run_mac=c8k${macid}
KC=200
KMV=100
MAXNUM=100
TrainRange=0-132743
TestRange=132744-162007
save_name=${exp_name}_graph_${JT}_test_norm
EXTRA=--force-shuffle=1
/opt/visal/tmp/for_sijin/Data/saved/Test/FCJ0_act_14_graph_0030_test_norm/
}



RUN {
exp_name=FCJ0_act_14
DP=mem
macid=13
JT=0029
EP=200
BSIZE=1024
run_mac=c8k${macid}
KC=200
KMV=100
MAXNUM=100
TrainRange=0-132743
TestRange=132744-162007
save_name=${exp_name}_graph_${JT}_test_norm
EXTRA=--force-shuffle=1 --cumulate-update
/opt/visal/tmp/for_sijin/Data/saved/theano_models/FCJ0_act_14_graph_0029_test_norm_cumulate_update/

/opt/visal/tmp/for_sijin/Data/saved/theano_models/FCJ0_act_14_graph_0029_test_norm_cumulate_update_eval/

}


Note {
exp_name=exp
DP=croppedimgcls
macid=15
JT=0032
EP=200
BSIZE=128
run_mac=c8k${macid}

TrainRange=0-12713
TestRange=12714-25428
# TrainRange=0-1023
# TestRange=1023-2047
# For Face++
TrainRange=0-17719
TestRange=17720-35440
test_freq=3
self.min_label = 20	
self.max_label = 59
self.num_classes = 40
/opt/visal/tmp/for_sijin/Data/saved/theano_models/Noah_face_Mar_6
Direct eval
7.8663732295%  data in [0, 1)
40.4266124937%  data in [0, 5)
77.1062581118%  data in [0, 10)
98.7303199594%  data in [0, 20)

--- With norm 
8.00180576717%  data in [0, 1)
40.2291067096%  data in [0, 5)
77.0328988206%  data in [0, 10)
98.5610292873%  data in [0, 20)
}

NOTE {
exp_name=ASM_act_5_exp_10
DP=croppedjt
macid=17
JT=0031
EP=200
BSIZE=128
run_mac=c8k${macid}
TrainRange=0-72435
TestRange=72436-103079
test_freq=15
save_name=Calculon_2015_03_03_${exp_name}_norm_JT_${JT}
/opt/visal/tmp/for_sijin/Data/saved/theano_models/Calculon_2015_03_03_ASM_act_5_exp_10_norm_JT_0031
mpjpe shape is (1, 128)
cur_mean is 179.952109197
Compute 239 batch 30592 data in total: 314.572048903 seconds 
mean mpjpe = 139.616395895
}

NOTE {
save evaluation of FCJ0_act_14_graph_0029_test_norm_eval
to /public/sijinli2/ibuffer/2015-03-06/FCJ0_act_14_graph_0029_test_norm_eval/eval_mpjpe
}

NOTE{
exp_name=ASM_act_14_exp_2
DP=croppedjt
macid=13
JT=0034
EP=200
BSIZE=64
run_mac=c8k${macid}
KC=200
KMV=100
MAXNUM=10
TrainRange=0-132743
TestRange=132744-162007
save_name=2015_03_07_tune_0034
EXTRA='--cumulate-update-num=5 --force-shuffle=1 --opt-method=ls'
COMMENTS: The performance is not good.
What is the problem?
/opt/visal/tmp/for_sijin/Data/saved/Test/2015_03_07_tune_0034
The saved version is the zero cost test version. 

}

NOTE {
exp_name=ASM_act_14_exp_2
DP=croppedjt
macid=13
JT=0035
EP=200
BSIZE=64
run_mac=c8k${macid}
KC=200
KMV=100
MAXNUM=10
TrainRange=0-132743
TestRange=132744-162007
save_name=2015_03_08_tune_0035
EXTRA='--cumulate-update-num=5 --force-shuffle=1 --opt-method=ls'
/opt/visal/tmp/for_sijin/Data/saved/Test/2015_03_08_tune_0035
}

NOTE {
exp_name=ASM_act_14_exp_2
DP=croppedjt
macid=13
JT=0036
EP=200
BSIZE=128
run_mac=c8k${macid}
KC=200
KMV=100
MAXNUM=10
TrainRange=0-132743
TestRange=132744-162007
save_name=2015_03_08_tune_0036
EXTRA='--cumulate-update-num=5 --force-shuffle=1 --opt-method=ls'
/opt/visal/tmp/for_sijin/Data/saved/Test/2015_03_08_tune_0036
COMMENTS: The performance on the training set is not good. 
}


NOTE {
exp_name=ASM_act_14_exp_2
DP=croppedjt
macid=13
JT=0037
EP=200
BSIZE=128
run_mac=c8k${macid}
KC=200
KMV=100
MAXNUM=10
TrainRange=0-132743
TestRange=132744-162007
save_name=2015_03_08_tune_0037
EXTRA='--cumulate-update-num=5 --force-shuffle=1 --opt-method=ls'
/opt/visal/tmp/for_sijin/Data/saved/Test/2015_03_08_tune_0037/
COMMENTS: The problem is that even on the training set, the performance is still not good.
}

NOTE {
exp_name=ASM_act_14_exp_2
DP=croppedjt
macid=13
JT=0038
EP=200
BSIZE=128
run_mac=c8k${macid}
KC=200
KMV=100
MAXNUM=10
TrainRange=0-132743
TestRange=132744-162007
save_name=2015_03_09_tune_0038
EXTRA='--cumulate-update-num=5 --force-shuffle=1 --opt-method=ls'
COMMENT: In this version, I only add the cosine cost layer,.
COMMENT: But I didnt' add the reconstruction cost
COMMENT: Then the network will only force it to learn "similar" embedding
COMMENT: It doesn't guarantee that the pose information is preserved.
}

NOTE {
It is embarasing that 0034 versin is not correct. 
I need to re-train 0034
}

NOTE {
exp_name=ASM_act_14_exp_2
DP=croppedjt
macid=13
JT=0034_corrected_version
EP=200
BSIZE=128
run_mac=c8k${macid}
KC=200
KMV=100
MAXNUM=10
TrainRange=0-132743
TestRange=132744-162007
save_name=2015_03_10_0034_corrected_version
EXTRA='--cumulate-update-num=5 --force-shuffle=1 --opt-method=ls'
/opt/visal/tmp/for_sijin/Data/saved/theano_models/2015_03_10_0034_corrected_version

[random batch exp, K= 20000, random2
Residuals for best match poses 52.2907453013
mpjpe as mv_margin is 87.3639631471
]
}

NOTE {
exp_name=ASM_act_14_exp_2
DP=croppedjt
macid=13
JT=0037_corrected_version
EP=200
BSIZE=128
run_mac=c8k${macid}
KC=200
KMV=100
MAXNUM=10
TrainRange=0-132743
TestRange=132744-162007
save_name=2015_03_10_0037_corrected_version
EXTRA='--cumulate-update-num=5 --force-shuffle=1 --opt-method=ls'
/opt/visal/tmp/for_sijin/Data/saved/theano_models/2015_03_10_0037_corrected_version/
[Random batch exp , K = 20000, random2
Residuals for best match poses 51.8139651997
mpjpe as mv_margin is 87.441633381
]
}


NOTE {
exp_name=ASM_act_14_exp_2
DP=croppedjt
macid=13
JT=0039
EP=200
BSIZE=256
run_mac=c8k${macid}
KC=200
KMV=100
MAXNUM=10
TrainRange=0-132743
TestRange=132744-162007
save_name=2015_03_12_0039_slack
EXTRA='--cumulate-update-num=5 --force-shuffle=1 --opt-method=ls'
EXE=/home/grads/sijinli2/pkg/anaconda/bin/python
Search_step_maximum = 1000
/opt/visal/tmp/for_sijin/Data/saved/Test/2015_03_12_0039_slack/
[random batch, 20000, 
Residuals for best match poses 52.2288127382
mpjpe as mv_margin is 88.8971149779
]
}

NOTE {
exp_name=ASM_act_14_exp_2
DP=croppedjt
macid=13
JT=0039_1
EP=200
BSIZE=512
run_mac=c8k${macid}
KC=200
KMV=100
MAXNUM=10
TrainRange=0-132743
TestRange=132744-162007
save_name=2015_03_13_0039_1_slack
EXTRA='--cumulate-update-num=20 --force-shuffle=1 --opt-method=ls'
EXE=/home/grads/sijinli2/pkg/anaconda/bin/python
Search_step_maximum = 100
/opt/visal/tmp/for_sijin/Data/saved/theano_models/2015_03_13_0039_1_slack/
[15@207
Residuals for best match poses 51.6113015437
mpjpe as mv_margin is 92.7454818498
]

[
epoch=37
write to /public/sijinli2/ibuffer/2015-03-13/2015_03_13_0039_1_slack_epoch_37/
Overall mpjpe is 91.211240498
]
}

NOTE {
exp_name=ASM_act_14_exp_2
DP=croppedjt
macid=13
JT=0039_2
EP=200
BSIZE=128
run_mac=c8k${macid}
KC=200
KMV=100
MAXNUM=10
TrainRange=0-132743
TestRange=132744-162007
save_name=2015_03_13_0039_2_slack
EXTRA='--cumulate-update-num=5 --force-shuffle=1 --opt-method=ls'
EXE=/home/grads/sijinli2/pkg/anaconda/bin/python
search_step_maximum=1000
/opt/visal/tmp/for_sijin/Data/saved/theano_models/2015_03_13_0039_2_slack/

[13.770 random batch, 20000
Residuals for best match poses 51.3779514205
mpjpe as mv_margin is 90.7690030649
]

}

NOTE {
exp_name=ASM_act_14_exp_2
DP=croppedjt
macid=13
JT=0039
EP=200
BSIZE=128
run_mac=c8k${macid}
KC=200
KMV=100
MAXNUM=10
TrainRange=0-132743
TestRange=132744-162007
save_name=2015_03_12_0039_slack_again
EXTRA='--cumulate-update-num=5 --force-shuffle=1 --opt-method=ls'
EXE=/home/grads/sijinli2/pkg/anaconda/bin/python

/opt/visal/tmp/for_sijin/Data/saved/theano_models/2015_03_12_0039_slack_again/
All is the same. 
However the training curve looks very different. 
The relative learning rate is important here.  
I think th emain problem here is the step size. 
When terminated accidently, I change the step size, then the performance became bad.
[16@686
Residuals for best match poses 52.8039261488
mpjpe as mv_margin is 90.5146398821
]
}

NOTE {
exp_name=ASM_act_14_exp_2
DP=croppedjt
macid=13
JT=0039_3
EP=200
BSIZE=128
run_mac=c8k${macid}
KC=200
KMV=100
MAXNUM=10
TrainRange=0-132743
TestRange=132744-162007
save_name=2015_03_13_0039_3_slack
EXTRA='--cumulate-update-num=5 --force-shuffle=1 --opt-method=ls'
EXE=/home/grads/sijinli2/pkg/anaconda/bin/python
Search_step_maximum = 1000
/opt/visal/tmp/for_sijin/Data/saved/Test/2015_03_13_0039_3_slack/
mpjpe as mv_margin is 96.789241189
search_step_maximum = 100
/opt/visal/tmp/for_sijin/Data/saved/theano_models/2015_03_13_0039_3_slack_100
[Wow, it is too slow here]
On epoch 79, the slack variable is stail 0.0259 ( * 1200, if for mpjpe explanation)

}


NOTE {
exp_name=ASM_act_14_exp_2
DP=croppedjt
macid=13
JT=0039
EP=200
BSIZE=128
run_mac=c8k${macid}
KC=200
KMV=100
MAXNUM=10
TrainRange=0-132743
TestRange=132744-162007
save_name=2015_03_12_0039_slack_again_100
EXTRA='--cumulate-update-num=7 --force-shuffle=1 --opt-method=ls'
EXE=/home/grads/sijinli2/pkg/anaconda/bin/python
/opt/visal/tmp/for_sijin/Data/saved/theano_models/2015_03_12_0039_slack_again_100/

NOTE {
epoch=25
/public/sijinli2/ibuffer/2015-03-13/2015_03_12_0039_slack_again_100_epoch_25
Overall mpjpe is 89.0587381611
}

NOTE {
epoch 34 for training
/public/sijinli2/ibuffer/2015-03-13/2015_03_12_0039_slack_again_100_epoch_34_train
}
}

RUN {
exp_name=ASM_act_14_exp_2
DP=croppedjt
macid=13
JT=0039_4
EP=200
BSIZE=128
run_mac=c8k${macid}
KC=200
KMV=100
MAXNUM=10
TrainRange=0-132743
TestRange=132744-162007
save_name=2015_03_12_0039_4_slack
EXTRA='--cumulate-update-num=7 --force-shuffle=1 --opt-method=ls'
EXE=/home/grads/sijinli2/pkg/anaconda/bin/python
/opt/visal/tmp/for_sijin/Data/saved/theano_models/2015_03_14_0039_4_slack/

saved to 
/public/sijinli2/ibuffer/2015-03-13/2015_03_14_0039_4_slack/


}

RUN {
exp_name=ASM_act_14_exp_2
DP=croppedjt
macid=13
JT=0039_5
EP=200
BSIZE=128
run_mac=c8k${macid}
KC=200
KMV=100
MAXNUM=10
TrainRange=0-132743
TestRange=132744-162007
save_name=2015_03_14_0039_5_slack
EXTRA='--cumulate-update-num=7 --force-shuffle=1 --opt-method=ls'
EXE=/home/grads/sijinli2/pkg/anaconda/bin/python
/opt/visal/tmp/for_sijin/Data/saved/Test/2015_03_14_0039_5_slack/
}


RUN {
exp_name=ASM_act_14_exp_2
DP=croppedjt
macid=13
JT=0040
EP=200
BSIZE=128
run_mac=c8k${macid}
KC=200
KMV=100
MAXNUM=10
TrainRange=0-132743
TestRange=132744-162007
save_name=2015_03_14_0040_slack
EXTRA='--cumulate-update-num=7 --force-shuffle=1 --opt-method=ls'
EXE=/home/grads/sijinli2/pkg/anaconda/bin/python
/opt/visal/tmp/for_sijin/Data/saved/theano_models/2015_03_14_0040_slack/

[Epoch 29 
saved to 
/public/sijinli2/ibuffer/2015-03-13/2015_03_14_0040_slack_epoch_29
Overall mpjpe is 90.7737477661
]

}

NOTE {
evaluation
epoch=37
/public/sijinli2/ibuffer/2015-03-13/2015_03_13_0039_1_slack_epoch_37/eval_mpjpe
Overall mpjpe is 91.211240498
}


RUN {
WARN:::::::
:::THIS IS NOT FOR TRAINING::::::::
:::I JUST WANT TO SEE WHETHER THE POOR PERFORMANCE IS CAUSED BY THE LACK OF TRAINING POSE:::
exp_name=ASM_act_14_exp_2
DP=croppedjt
macid=13
JT=0039
EP=200
BSIZE=128
run_mac=c8k${macid}
KC=200
KMV=100
MAXNUM=10
TrainRange=0-132743
TestRange=132744-162007
save_name=2015_03_15_0039_slack_with_test_pose
EXTRA='--cumulate-update-num=5 --force-shuffle=1 --opt-method=ls --candidate-mode=testrandom'
EXE=/home/grads/sijinli2/pkg/anaconda/bin/python
/opt/visal/tmp/for_sijin/Data/saved/theano_models/2015_03_15_0039_slack_with_test_pose/

COMMENTS: We didn't get improvement on the test set. It is weird.
}


RUN {

}